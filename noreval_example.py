# -*- coding: utf-8 -*-
"""noreval_example.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fq89q2GtgZQlZcA3FYGTW5GhhdO50mVm

# Evaluating Hugging Face Models on the NorEval Benchmark

This notebook evaluates Hugging Face models (e.g., Google Gemma 3, Mistral, Llama) on tasks within the NorEval benchmark using the `lm-evaluation-harness`.

**How To Use:**

1.  **Prerequisites:**
    * You need a **Hugging Face Account**.
    * Generate a **Hugging Face Access Token** (`Settings -> Access Tokens -> New token` with `read` role) if you plan to use gated models (like Gemma, Llama 3).
    * For gated models, go to their Hugging Face page (e.g., [google/gemma-3-1b-it](https://huggingface.co/google/gemma-3-1b-it)) and **accept the terms of use**.
    * Ensure you are using a **GPU runtime** in Colab (Runtime -> Change runtime type -> T4 GPU or better is recommended for most models).

2.  **Install Dependencies:**
    * Execute **"Step 1: Install Dependencies"**. This installs necessary libraries like `transformers`, `lm-evaluation-harness`, etc.

3.  **Setup Hugging Face Authentication (if needed):**
    * Run the **"Step 2: Setup Hugging Face Authentication"** cell.
    * Follow the instructions printed by the cell to add your Hugging Face token (`HF_TOKEN`) to Colab's Secrets Manager (key icon ðŸ”‘ on the left). This is required for gated models.

4.  **Clone NorEval Repository:**
    * Execute **"Step 3: Clone NorEval Repository"**. This downloads the benchmark task configurations.

5.  **Configure and Run Evaluation:**
    * Go to **"Step 4: Configure and Run Evaluation..."**.
    * **Select the Hugging Face Model ID** you want to evaluate (e.g., `google/gemma-3-1b-it`, `meta-llama/Meta-Llama-3-8B-Instruct`).
    * **Select the NorEval Task(s)** (e.g., `norquad`, `nrk_quiz_qa_nob`, etc.). Find task names in the `./noreval/tasks` folder after running Step 3.
    * Adjust precision, quantization, few-shot count, batch size, and output directory as needed using the form.
    * Run the cell. This step will download the model (if not cached) and perform the evaluation. **This can take a significant amount of time.**

6.  **(Optional) View Raw Output Files:**
    * Run **"Step 5: View Raw Output Files (Optional)"** to list the contents of the output directory created in Step 4, including the timestamped results file and sample files.

7.  **Analyze Results:**
    * Once Step 4 is complete, run **"Step 6: Analyze and Present Results"**. This will find and load the metrics from the generated timestamped results file (e.g., `results_*.json`) and display them in a summary table.


NorEval paper: https://arxiv.org/pdf/2504.07749

NorEval github repo: https://github.com/ltgoslo/noreval
"""

# @title ## Step 1: Install Dependencies
# Install necessary libraries, including a recent version of `transformers` (>=4.50.0) required for Gemma 3,
# and the latest version of `lm-evaluation-harness` to ensure compatibility.

# Install transformers >= 4.50.0
!pip install --quiet "transformers>=4.50.0"

# Install latest lm-evaluation-harness from GitHub
!pip install --quiet git+https://github.com/EleutherAI/lm-evaluation-harness.git

# Install other necessary dependencies
!pip install --quiet torch accelerate bitsandbytes datasets sentencepiece protobuf huggingface_hub hf_transfer

# Verify installations (optional)
!pip show transformers lm_eval

print("\n--- Dependencies installed (using latest lm-eval & recent transformers) ---")
print("--- Note: Using latest lm-eval instead of v0.4.8 due to transformers>=4.50.0 requirement for Gemma 3 ---")

# @title ## Step 2: Setup Hugging Face Authentication
# Gemma 3 models are gated. You need a Hugging Face token stored in Colab Secrets.
# Ensure you have accepted the Gemma 3 model's terms on its Hugging Face page (e.g., google/gemma-3-1b-it).

# Import necessary libraries
import os
from google.colab import userdata
from huggingface_hub import login

print("--- Setup Instructions for Hugging Face Token ---")
print("1. Go to your Hugging Face account settings -> Access Tokens.")
print("2. Create a new token with 'read' permissions.")
print("3. In this Colab notebook, click the key icon (ðŸ”‘) in the left sidebar -> 'Add new secret'.")
print("4. Name the secret 'HF_TOKEN' and paste your Hugging Face token into the 'Value' field.")
print("5. Make sure 'Notebook access' is toggled ON for the secret.")
print("-------------------------------------------------")

# Attempt to load the Hugging Face token from Colab secrets
try:
    hf_token = userdata.get('HF_TOKEN')
    if hf_token:
        print("Hugging Face token found in Colab secrets.")
        # Log in to Hugging Face Hub
        login(token=hf_token, add_to_git_credential=False)
        print("Successfully logged into Hugging Face Hub.")
    else:
        print("Warning: Hugging Face token not found. You might encounter issues downloading gated models like Gemma 3.")
        print("Please add your HF_TOKEN via the 'Secrets' tab (key icon on the left).")
except Exception as e:
    print(f"An error occurred during Hugging Face login: {e}")
    print("Please ensure your HF_TOKEN secret is correctly set up and you have accepted the model terms on Hugging Face.")

# Check GPU availability (recommended)
import torch
if torch.cuda.is_available():
    print(f"\nGPU found: {torch.cuda.get_device_name(0)}")
    print("Running on GPU is recommended.")
else:
    print("\nWarning: No GPU detected. Running Gemma 3 on CPU will be very slow and may fail due to memory limits.")
    print("Consider enabling a GPU runtime (Runtime -> Change runtime type -> Hardware accelerator -> T4 GPU or higher).")

# @title ## Step 3: Clone NorEval Repository
# Clone the NorEval repository which contains the task configurations
!git clone https://github.com/ltgoslo/noreval.git

# Check if the directory exists
!ls

# @title ## Step 4: Configure and Run Evaluation with Official Gemma 3

# @markdown ### Model Configuration:
# @markdown Select the official Google Gemma 3 model ID from Hugging Face.
gemma3_model_id = "google/gemma-3-1b-it" #@param ["google/gemma-3-1b-it", "google/gemma-3-4b-it", "google/gemma-3-12b-it", "google/gemma-3-27b-it"]
# @markdown Set precision. `bfloat16` is generally recommended for newer GPUs.
precision = "bfloat16" #@param ["bfloat16", "float16", "float32"]
# @markdown Optional: Use 8-bit quantization to reduce memory (requires `bitsandbytes`).
use_8bit_quantization = False #@param {type:"boolean"}
# @markdown ---
# @markdown ### NorEval Task Configuration:
# @markdown Specify the NorEval task(s) to run (e.g., `norquad`).
noreval_task = "nrk_quiz_qa_nob" #@param {type:"string"}
num_fewshot = 0 #@param {type:"integer"}
# @markdown ---
# @markdown ### Evaluation Settings:
batch_size = "auto" #@param ["auto", "1", "2", "4", "8"] {allow-input: true}
output_dir = f"results_{gemma3_model_id.split('/')[-1].replace('-', '_')}/{noreval_task}/{num_fewshot}-shot/" #@param {type:"string"}

# @markdown ---
# @markdown Now, run the cell below to start the evaluation.

# Construct model arguments
model_args = f"pretrained={gemma3_model_id},trust_remote_code=True,dtype={precision}"
if use_8bit_quantization:
    model_args += ",load_in_8bit=True"
    print("Note: Using 8-bit quantization.")

print(f"\nStarting evaluation for model: {gemma3_model_id}")
print(f"Task(s): {noreval_task}")
print(f"Precision: {precision}")
print(f"Few-shot examples: {num_fewshot}")
print(f"Batch size: {batch_size}")
print(f"Output directory: {output_dir}")
print(f"Model Args: {model_args}")
print("\nThis may take some time...")

# Construct and run the lm_eval command
!lm_eval \
  --model hf \
  --model_args {model_args} \
  --tasks {noreval_task} \
  --include_path ./noreval/ \
  --output {output_dir} \
  --log_samples \
  --batch_size {batch_size} \
  --num_fewshot {num_fewshot} \
  --show_config \
  --write_out

print("\n--- Evaluation finished ---")

# @title ## Step 5: View Results
# The evaluation results are saved in the specified output directory.
# You can use the file browser on the left sidebar of Colab to navigate into the directory
# (you might need to refresh it) and view/download the JSON files.

# List the contents of the output directory
print(f"\nContents of the output directory: {output_dir}")
# Use !ls -R to recursively list contents if needed
!ls -lh {output_dir}

# @title ## Step 6: Analyze and Present Results
# Here we load the timestamped `results_*.json` file from the evaluation output
# (accounting for the model-specific subdirectory)
# and present the metrics in a clean table using pandas.

import json
import pandas as pd
import os # Ensure os is imported
import glob # Import glob to find files matching a pattern

# Ensure output_dir and gemma3_model_id are defined (should be from Step 4)
if 'output_dir' not in globals() or 'gemma3_model_id' not in globals():
    print("Error: 'output_dir' or 'gemma3_model_id' variable not found. Please run Step 4 first.")
else:
    # Construct the path to the model-specific subdirectory
    model_subdir_name = gemma3_model_id.replace('/', '__')
    results_dir_path = os.path.join(output_dir, model_subdir_name)

    print(f"Looking for results file in directory: {results_dir_path}")

    # Use glob to find the results file (matching results_*.json)
    results_file_pattern = os.path.join(results_dir_path, "results_*.json")
    found_files = glob.glob(results_file_pattern)

    if not found_files:
        print(f"Error: No results file matching 'results_*.json' found in '{results_dir_path}'")
        print("Please ensure Step 4 completed successfully and generated the expected output structure.")
    elif len(found_files) > 1:
        print(f"Error: Multiple results files found in '{results_dir_path}'. Please check the directory.")
        print(found_files)
    else:
        results_file_path = found_files[0] # Get the first (and only) matched file path
        print(f"Loading results from: {results_file_path}")

        # Load the results JSON file
        try:
            with open(results_file_path, 'r') as f:
                results_data = json.load(f)

            # Extract configuration details for context
            config = results_data.get('config', {})
            # Handle potential variations in how model args are stored
            model_args_val = config.get('model_args', {})
            if isinstance(model_args_val, dict):
                 model_used = ", ".join(f"{k}={v}" for k, v in model_args_val.items())
            elif isinstance(model_args_val, str): # Handle case where it might be a string already
                 model_used = model_args_val
            else:
                 model_used = 'N/A'

            tasks_run = config.get('tasks', 'N/A')
            num_fewshot_run = config.get('num_fewshot', 'N/A')
            # --- End Updated Config Extraction ---


            print("\n--- Evaluation Configuration ---")
            print(f"Model Args: {model_used}")
            print(f"Tasks: {tasks_run}")
            print(f"Few-shot: {num_fewshot_run}")
            print("--------------------------------\n")

            # Prepare data for pandas DataFrame
            table_data = []
            results = results_data.get('results', {})

            for task_name, metrics in results.items():
                if isinstance(metrics, dict):
                    # Process only main metrics (not stderr variants)
                    for metric_name_full, value in metrics.items():
                        # Check if it's a primary metric key (doesn't contain '_stderr')
                        if '_stderr' not in metric_name_full and not metric_name_full.endswith('_samples') and isinstance(value, (int, float)):
                            # Construct the corresponding stderr key name
                            # Assumes format "metric,filter" -> "metric_stderr,filter"
                            parts = metric_name_full.split(',', 1)
                            if len(parts) == 2:
                                stderr_key = f"{parts[0]}_stderr,{parts[1]}"
                            else:
                                # Fallback if no comma (less likely for lm-eval results)
                                stderr_key = f"{metric_name_full}_stderr"

                            # Get the stderr value using the correctly constructed key
                            stderr_value = metrics.get(stderr_key, 'N/A')

                            table_data.append({
                                "Task": task_name,
                                # Use the original full metric name (e.g., "f1,none")
                                "Metric": metric_name_full,
                                "Value": f"{value:.4f}",
                                "StdErr": f"{stderr_value:.4f}" if isinstance(stderr_value, float) else stderr_value
                            })
                else:
                     print(f"Skipping non-dictionary metric entry for task '{task_name}': {metrics}")


            if not table_data:
                print("No metric data parsed from the results file. Check JSON structure if evaluation succeeded.")
            else:
                # Create DataFrame
                df = pd.DataFrame(table_data)

                # Display the DataFrame
                print("--- Evaluation Results ---")
                from IPython.display import display
                display(df)

                # Optional: Save DataFrame to CSV in the *main* output directory
                try:
                    csv_path = os.path.join(output_dir, "results_summary.csv")
                    df.to_csv(csv_path, index=False)
                    print(f"\nResults summary also saved to: {csv_path}")
                except Exception as e:
                    print(f"\nCould not save results summary to CSV: {e}")

        except Exception as e:
            print(f"An error occurred while loading or processing the results file '{results_file_path}': {e}")
            import traceback
            traceback.print_exc() # Print detailed error traceback